{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11478045,"sourceType":"datasetVersion","datasetId":7190544}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load Pre-Trained BLIP for Image Captioning","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n# Load captioning model\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:05:03.390447Z","iopub.execute_input":"2025-04-19T20:05:03.391275Z","iopub.status.idle":"2025-04-19T20:05:44.087567Z","shell.execute_reply.started":"2025-04-19T20:05:03.391238Z","shell.execute_reply":"2025-04-19T20:05:44.086356Z"}},"outputs":[{"name":"stderr","text":"2025-04-19 20:05:21.300431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745093121.537878      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745093121.610720      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f10b0a264448f482b28ae3f7d64c61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0920d45f8548e0befd6ea0064c0168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3787f923ee0346608b47fc89e2a34b50"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f5d01d99e84306b895d15feedf6e5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6070515c3ee432c8a16633717cd8e9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d465b1af5ad41839839cdddf55bef8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c7c83ec9614628bbc77b21a26932c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2ddcf601574c979fa3a686173a1081"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Generate Caption for each image","metadata":{}},{"cell_type":"code","source":"captions = dict()\n#Loop through all images in folder\nfor filename in os.listdir(\"/kaggle/input/cv-assgn3/samples\"):\n    image_path = os.path.join(\"/kaggle/input/cv-assgn3/samples\", filename)\n    image = Image.open(image_path).convert(\"RGB\")\n\n    # Preprocess and caption\n    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device)\n    with torch.no_grad():\n        output = blip_model.generate(**inputs)\n        caption = blip_processor.decode(output[0], skip_special_tokens=True)\n\n    captions[image_path] = caption\n\ncaptions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:06:30.028745Z","iopub.execute_input":"2025-04-19T20:06:30.029088Z","iopub.status.idle":"2025-04-19T20:07:03.096430Z","shell.execute_reply.started":"2025-04-19T20:06:30.029059Z","shell.execute_reply":"2025-04-19T20:07:03.095182Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000004.jpg': 'a small dog running across a green field',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000022.jpg': 'a small dog standing on a stone ledge',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000023.jpg': 'a man riding a bike down a wet street',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000026.jpg': 'a man in a suit and tie sitting on a couch',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000018.jpg': 'a family sitting in a pool with a towel',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000003.jpg': 'a small dog walking on a green carpet',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000019.jpg': 'a small bird sitting on a plant',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000030.jpg': 'a duck drinking water from a pond',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000034.jpg': 'a coffee machine with two cups on it',\n '/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000025.jpg': 'a brown butterfly sitting on a green plant'}"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 3. Evaluation using CLIP","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\n\n# Load CLIP \nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nfor image_path, caption in captions.items():\n    image = Image.open(image_path).convert(\"RGB\")\n\n    clip_inputs = clip_processor(text=[caption], images=image, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        clip_outputs = clip_model(**clip_inputs)\n        # Get features \n        image_features = clip_outputs.image_embeds\n        text_features = clip_outputs.text_embeds\n        # Normalize features\n        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n        # Compute cosine similarity\n        similarity = (100*image_features @ text_features.T).item()\n\n    # ----- Output -----\n    print(f\"{image_path} | CLIP Similarity: {similarity:.4f} \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:24:48.787698Z","iopub.execute_input":"2025-04-19T20:24:48.788484Z","iopub.status.idle":"2025-04-19T20:24:52.300171Z","shell.execute_reply.started":"2025-04-19T20:24:48.788442Z","shell.execute_reply":"2025-04-19T20:24:52.299429Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000004.jpg | CLIP Similarity: 32.7026 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000022.jpg | CLIP Similarity: 31.0347 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000023.jpg | CLIP Similarity: 30.8345 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000026.jpg | CLIP Similarity: 28.8953 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000018.jpg | CLIP Similarity: 31.3365 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000003.jpg | CLIP Similarity: 31.5660 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000019.jpg | CLIP Similarity: 28.9383 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000030.jpg | CLIP Similarity: 30.5252 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000034.jpg | CLIP Similarity: 27.9613 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000025.jpg | CLIP Similarity: 28.9163 \n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## 4. Evaluation using CLIPS","metadata":{}},{"cell_type":"code","source":"!pip install open_clip_torch --quiet\nfrom open_clip import create_model_from_pretrained, get_tokenizer\n\n# Load CLIP \nclips_model, clips_preprocess = create_model_from_pretrained(\"hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B\")\nclips_tokenizer = get_tokenizer(\"hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B\")\n\nfor image_path, caption in captions.items():\n    image = Image.open(image_path).convert(\"RGB\")\n\n    image_input = clips_preprocess(image).unsqueeze(0)\n    text_input = clips_tokenizer([caption])\n\n    with torch.no_grad():\n        \n        # Get features correctly\n        image_features = clips_model.encode_image(image_input)\n        text_features = clips_model.encode_text(text_input)\n        # Normalize features\n        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n        # Compute cosine similarity\n        similarity = (100*image_features @ text_features.T).item()\n\n    # ----- Output -----\n    print(f\"{image_path} | CLIPS Similarity: {similarity:.4f} \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:34:56.237001Z","iopub.execute_input":"2025-04-19T20:34:56.237517Z","iopub.status.idle":"2025-04-19T20:35:29.508630Z","shell.execute_reply.started":"2025-04-19T20:34:56.237469Z","shell.execute_reply":"2025-04-19T20:35:29.507595Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000004.jpg | CLIPS Similarity: 19.1549 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000022.jpg | CLIPS Similarity: 15.1255 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000023.jpg | CLIPS Similarity: 16.9236 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000026.jpg | CLIPS Similarity: 12.7573 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000018.jpg | CLIPS Similarity: 14.8474 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000003.jpg | CLIPS Similarity: 18.1808 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000019.jpg | CLIPS Similarity: 16.9768 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000030.jpg | CLIPS Similarity: 16.5172 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000034.jpg | CLIPS Similarity: 15.6065 \n/kaggle/input/cv-assgn3/samples/ILSVRC2012_test_00000025.jpg | CLIPS Similarity: 16.7442 \n","output_type":"stream"}],"execution_count":28}]}