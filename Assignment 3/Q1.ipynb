{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11473436,"sourceType":"datasetVersion","datasetId":7190544}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. CLIP dependendy ","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch torchvision  open_clip_torch --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T14:51:09.863755Z","iopub.execute_input":"2025-04-19T14:51:09.867058Z","iopub.status.idle":"2025-04-19T14:51:15.211239Z","shell.execute_reply.started":"2025-04-19T14:51:09.866831Z","shell.execute_reply":"2025-04-19T14:51:15.209385Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 2. Download CLIP Model and Load Weights","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nimport torch.nn.functional as F\nimport torch\nfrom PIL import Image\nimport requests\n\n# Load model and processor\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:20:45.496214Z","iopub.execute_input":"2025-04-19T15:20:45.496567Z","iopub.status.idle":"2025-04-19T15:20:47.554611Z","shell.execute_reply.started":"2025-04-19T15:20:45.496542Z","shell.execute_reply":"2025-04-19T15:20:47.553573Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## 3. Generate CLIP Similarity Scores","metadata":{}},{"cell_type":"code","source":"# Load sample image (replace with your image path or URL)\nimage = Image.open(\"/kaggle/input/cv-assgn3/sample_image.jpg\").convert(\"RGB\")\n\n# 10 custom textual descriptions\ntexts = [\n    \"A man holding a dog\",\n    \"A woman holding a dog\",\n    \"A man holding an elephant\",\n    \"A woman holding an elephant\",\n    \"A man cooking in a kitchen\",\n    \"A beautiful sunset\",\n    \"A person sitting on a bench\",\n    \"A dog and its owner\",\n    \"A city skyline at night\",\n    \"A Room full of toys\",\n    \"A dog looking at a human\"\n]\n\n# Preprocess\ninputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n\n# Forward pass\noutputs = model(**inputs)\n\n# Get image and text embeddings\nimage_embeds = outputs.image_embeds\ntext_embeds = outputs.text_embeds\n\n# Normalize\nimage_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\ntext_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n\n# Cosine similarity\nsimilarity_scores = (100*image_embeds @ text_embeds.T).squeeze().tolist()\n\n# Print results\nfor desc, score in zip(texts, similarity_scores):\n    print(f\"{desc} --> Similarity Score: {score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:20:47.555997Z","iopub.execute_input":"2025-04-19T15:20:47.556344Z","iopub.status.idle":"2025-04-19T15:20:48.024078Z","shell.execute_reply.started":"2025-04-19T15:20:47.556314Z","shell.execute_reply":"2025-04-19T15:20:48.023101Z"}},"outputs":[{"name":"stdout","text":"A man holding a dog --> Similarity Score: 30.5882\nA woman holding a dog --> Similarity Score: 26.2149\nA man holding an elephant --> Similarity Score: 26.6754\nA woman holding an elephant --> Similarity Score: 21.1037\nA man cooking in a kitchen --> Similarity Score: 21.0087\nA beautiful sunset --> Similarity Score: 16.3746\nA person sitting on a bench --> Similarity Score: 20.0851\nA dog and its owner --> Similarity Score: 27.6654\nA city skyline at night --> Similarity Score: 12.8141\nA Room full of toys --> Similarity Score: 19.5991\nA dog looking at a human --> Similarity Score: 25.2832\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## 4. CLIPS dependency","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/UCSC-VLAA/\n# CLIPS.git\n# !pip install -r CLIPS/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T14:51:17.821110Z","iopub.execute_input":"2025-04-19T14:51:17.821524Z","iopub.status.idle":"2025-04-19T14:51:17.826004Z","shell.execute_reply.started":"2025-04-19T14:51:17.821497Z","shell.execute_reply":"2025-04-19T14:51:17.824926Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 5. Download CLIPS Model and Load Weight","metadata":{}},{"cell_type":"code","source":"# Load model\nmodel, preprocess = create_model_from_pretrained(\"hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B\")\ntokenizer = get_tokenizer(\"hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:21:03.387715Z","iopub.execute_input":"2025-04-19T15:21:03.388104Z","iopub.status.idle":"2025-04-19T15:21:19.696383Z","shell.execute_reply.started":"2025-04-19T15:21:03.388077Z","shell.execute_reply":"2025-04-19T15:21:19.695380Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## 6. Generate CLIPS Similarity Scores","metadata":{}},{"cell_type":"code","source":"# Preprocess\nimage_input = preprocess(image).unsqueeze(0)  \ntext_input = tokenizer(texts)\n\n# Forward pass\nwith torch.no_grad():\n    image_embeds = model.encode_image(image_input)\n    text_embeds = model.encode_text(text_input)\n\n# Normalize\nimage_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\ntext_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n\n# Cosine similarity\nsimilarity_scores = (100*image_embeds @ text_embeds.T).squeeze().tolist()\n\n# Print results\nfor desc, score in zip(texts, similarity_scores):\n    print(f\"{desc} --> Similarity Score: {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:21:19.698532Z","iopub.execute_input":"2025-04-19T15:21:19.698844Z","iopub.status.idle":"2025-04-19T15:21:22.735802Z","shell.execute_reply.started":"2025-04-19T15:21:19.698821Z","shell.execute_reply":"2025-04-19T15:21:22.734712Z"}},"outputs":[{"name":"stdout","text":"A man holding a dog --> Similarity Score: 16.3031\nA woman holding a dog --> Similarity Score: 10.1649\nA man holding an elephant --> Similarity Score: 16.4388\nA woman holding an elephant --> Similarity Score: 10.8382\nA man cooking in a kitchen --> Similarity Score: 6.4267\nA beautiful sunset --> Similarity Score: 3.7355\nA person sitting on a bench --> Similarity Score: -0.2385\nA dog and its owner --> Similarity Score: 15.3522\nA city skyline at night --> Similarity Score: -0.2308\nA Room full of toys --> Similarity Score: 6.8924\nA dog looking at a human --> Similarity Score: 12.7015\n","output_type":"stream"}],"execution_count":26}]}